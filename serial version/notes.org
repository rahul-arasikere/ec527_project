
* Tasks
  
** basic goal: 4 segments
** TODO mess around with filters and do better pre-processing
** TODO look at other implementations
** get video files working
** make function call that only converts one frame and returns a segmented return
** TODO look into contours, gaussian, and laplace filters
** TODO also look into and understand the watershed algorithm
* Notes
** Watershed Algorithm
*** Problems with the Watershed Algorithm
The watershed algorithm is non-parametric, this means you do not need to put in any parameters when first applying it to an image. However, in order to get reasonable results, you would have to use what it called the "waterfall transformation", which takes in the initial output of the initial application of the watershed algorithm and further simplifies it. Below, is an example of this application:
#+DOWNLOADED: screenshot @ 2021-03-25 23:57:14
[[file:images/20210325-235714_screenshot.png]]

Do you think /(c)/, /(d)/, or /(e)/ is better? The problem with the waterfall transformation is that it either simplifies an image too much or too little, and there is nothing we can do to help it a priori to get the desired result. Also, none of the images produced actually were able to properly segment the tools and keys depicted, and this is a relatively simple image. This problem is called "waterfall transformation shortsightedness", as it is hard to know how well it will segment the image until it is applied.
*** An overview of the waterfall transformation 
The waterfall transformation basically will look at all basins of an image (generated from the watershed algorithm), and number them. It will start with one of these arbitrary basins and fill it until it floods into another basin. Then, it will flood the basin that was flooded into. If this second basin floods back into the original one, then this is symmetrical flooding. This means that these segments of the image were very similar in average value, and therefore can be grouped into a larger basin.

#+DOWNLOADED: screenshot @ 2021-03-26 01:10:01
[[file:images/20210326-011001_screenshot.png]]
In Fig 10 (g), you can see a 2d representation of the WT at work, slowly flooding and combining basins. In (c), you can see the first time where the second overflowed basin (basin 7) does not flow back into the original (a mega basin of 1-6). This means that this boundary should be preserved as a WL (watershed line).  It will then restart the process of filling basins from basin 7. This will be done to the entire image and you will be left with (f), a watershed-transformed watershed image. 

*** Rejoice! The "P Algorithm"
The "P Algorithm" works generally the same way as the waterfall transformation, except that it re-introduces contours later into the re-application of itself, ensuring that features don't get too "washed out". It is kind of hard to explain... but it basically ensures that maximal "islands" which are not directly connected to a contour edge from the watershed gradient of the image do not get washed away into the oblivion but instead re-introduces them later.
#+DOWNLOADED: screenshot @ 2021-03-26 01:02:14
[[file:images/20210326-010214_screenshot.png]]

Look at how much better it performs! 

The only drawback is that it takes many times more iterations to produce a desired result. In simpler cases though, it has been shown to have the same (or nearly the same) amount of iterations as the waterfall transformation.

Another problem, is that it still does not address the threshold problem faced by the waterfall transform. Unfortunately, you would have to test your image and multiple thresholds. Then, compare each successive result to the last, and when you get repeated images, that is normally the desired threshold, as it is now "stable".

It also is not so great at handling textured regions, often re-introducing too many contours. Trees and bushes are a good example of where this algorithm is not performant.

*** Sources
**** http://www.cmm.mines-paristech.fr/~beucher/wtshed.html
**** http://cmm.ensmp.fr/%7Ebeucher/publi/P-Algorithm_SB_BM.pdf
** Image Filtering and convolution
*** Image Kernels
**** What is a kernel?
Whenever you have an image, you can apply something called a "kernel". A kernel is just a function that you apply to multiple pixels at the same time. Most kernels are 3x3 but can come in other (normally square) shapes. When a kernel is 3x3, you are simply defining a function to take in the surrounding pixels of the pixel you are currently modifying. With a 5x5, you are defining a function that takes the outer-two layers of the pixel you are modifying as input. Each kernel application only modifies one pixel at a time. Different kernels have different effects.
- Stride is the amount the kernel steps by each time it is applied; it is normally applied right-to-left, top-to-bottom.
#+DOWNLOADED: screenshot @ 2021-03-26 03:13:23
[[file:images/20210326-031323_screenshot.png]]
**** But what about the edge values?
One problem with applying the kernel, is that when you get to the edge pixels, you would not be able to get the inputs you need to modify it from the original image. The surrounding pixels of a pixel on the edge of an image is nonexistent, the kernel doesn't work. One workaround is to just add a padding of black pixels to the edge of your image, but this can mess up many algorithms:
#+DOWNLOADED: screenshot @ 2021-03-26 03:19:53
[[file:images/20210326-031953_screenshot.png]]
Fortunately, there are many other methods of padding which allow us to extend the edge of our image in ways that make sense.
- If you don't pad your image, then the output image will always be smaller than the input image.
**** Image Data Types
While you may have an input image with pixels of type unsigned, 8-bit integer, you will find after applying the kernel, that you may get fractional values and values that go above the 8-bit threshold. So should you convert your image to 16-bit? No, the best method is to simply convert your image to floating point, and represent values from the range 0...1. This way, values above 255 would just be converted to a fractional representation and would not be clipped. Additionally, fractional values would just have to be scaled in the range, and would lose no precision.
**** Example: Applying a Kernel with Python
#+DOWNLOADED: screenshot @ 2021-03-26 03:50:19
[[file:images/20210326-035019_screenshot.png]]
*** Unsharp Mask
The main idea is to apply a gaussian kernel and subtract that from the original image, this is the unsharp mask. With this, when added to the original image, you get a sharpened image.
#+begin_equation
S = original + (original - gaussian) * intensity
#+end_equation
...and here is the code to do that.
#+DOWNLOADED: screenshot @ 2021-03-26 03:55:55
[[file:images/20210326-035555_screenshot.png]]
Another method:
#+DOWNLOADED: screenshot @ 2021-03-26 03:58:52
[[file:images/20210326-035852_screenshot.png]]
...because many libraries just have this functionality built in.
*** Median Filter
Gaussian Filter is great for denoising blurry images, but terrible and de-noising salt and pepper noise. The median filter works by replacing the current pixel being filtered with the median of the kernel. 
*** Edge Filters
Most all edge filters use convolutions on your image. Some popular edge filters include: Roberts, Sobel, Scharr, Prewitt, Farid, and Canny. Other image filters use a frequency domain; this entails applying a DFT to produce a frequency domain and then applying a mask.
**** Roberts - 2x2 kernel designed to approximate the gradient or derivative of the image
**** Sobel - nearly the same as Roberts but a 3x3 matrix instead
The kernel actually changes based on if it is going in the x-direction or the y-direction. If it is goin in the x-direction, the kernel is:
| -1 | 0 | 1 |
| -2 | 0 | 2 |
| -1 | 0 | 1 |
Studying it, you can see it is just doing a subtraction of the right column minus the left column. For the y-direction, the kernel is
| -1 | -2 | -1 |
| 0  |  0 |  0 |
|  1 |  2 |  1 |
After doind both of these, it then combines them like so:
#+begin_equation
G = √(Gₓ² + G_y²)
#+end_equation
Where /G/ gives the overall intensity of the edges in the image.

To get the orientation of the edge (because each edge now basically has an x and y component), it can be done using:
#+begin_equation
arctan(\frac{G_y}{Gₓ})
#+end_equation

Here is an image that had sobel-edge detection applied and then the x and y directions combined:
#+DOWNLOADED: screenshot @ 2021-03-26 05:49:31
[[file:images/20210326-054931_screenshot.png]]
Notice how they were able to color the edges based on the calculated angle.

It is very common to use a Gaussian blur first to get rid of noise in the image that would mess up the Sobel kernel because it is so small.
**** Scharr - designed to find gradients along x- and y-axis
**** Prewitt - another Roberts clone
**** Farid - proposes two different pairs of kernels, one for interpolation (5x5) and another for differentiation (7x7).
**** Canny - Gives Sobel steroids
Canny edge detection takes in the Sobel kernel as input (with the angles of each edge). It starts by thining all the edges so that they are 1px wide.

Then, it works to remove unecessary and unwanted edges through hysteresis thresholding.
#+DOWNLOADED: screenshot @ 2021-03-26 05:57:02
[[file:images/20210326-055702_screenshot.png]]
*** Thresholding
Thesholding is just a function that bound pixels to 0 or 1 based on a minimum value. If the pixel is below the value, it is set to 0; above, it is set to 1.
**** Simple example
#+begin_src python
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread('gradient.png',0)
ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)
ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)
ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)
ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)
titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
for i in range(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
#+end_src
The code produces this:
#+DOWNLOADED: screenshot @ 2021-03-26 11:48:30
[[file:images/20210326-114830_screenshot.png]]
These methods, however, need a constant, global value. This is not very helpful when you are trying to apply this to many images or when you cannot be spending time figuring it out for each image. 
**** Rejoice! Adaptive Thresholding
This algorithm works well on complex images and images that have different lighting in different areas. It defines a threshold for a pixel based on the pixels around it.
#+begin_src python
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread('sudoku.png',0)
img = cv.medianBlur(img,5)
ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\
            cv.THRESH_BINARY,11,2)
th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\
            cv.THRESH_BINARY,11,2)
titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']
images = [img, th1, th2, th3]
for i in range(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
#+end_src
Result:

#+DOWNLOADED: screenshot @ 2021-03-26 11:56:41
[[file:images/20210326-115641_screenshot.png]]
**** Otsu Binarization
This last technique is only good on image where you are trying to separate a foreground from a backgronud. It does not make sense to run this algorithm outside of this scope. To be more specific, this algorithm is perfect on images that have a bimodal distribution of the histogram:
#+DOWNLOADED: screenshot @ 2021-03-26 11:59:26
[[file:images/20210326-115926_screenshot.png]]
In this case, a good threshold value would be between these two peaks.
#+begin_src python
import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread('noisy2.png',0)
# global thresholding
ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
# Otsu's thresholding
ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)
# Otsu's thresholding after Gaussian filtering
blur = cv.GaussianBlur(img,(5,5),0)
ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)
# plot all the images and their histograms
images = [img, 0, th1,
          img, 0, th2,
          blur, 0, th3]
titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',
          'Original Noisy Image','Histogram',"Otsu's Thresholding",
          'Gaussian filtered Image','Histogram',"Otsu's Thresholding"]
for i in range(3):
    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')
    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)
    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')
    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])
plt.show()
#+end_src
Result:
#+DOWNLOADED: screenshot @ 2021-03-26 12:01:03
[[file:images/20210326-120103_screenshot.png]]
***** DIY Implementation of Otsu Binarization
#+begin_src python
img = cv.imread('noisy2.png',0)
blur = cv.GaussianBlur(img,(5,5),0)
# find normalized_histogram, and its cumulative distribution function
hist = cv.calcHist([blur],[0],None,[256],[0,256])
hist_norm = hist.ravel()/hist.sum()
Q = hist_norm.cumsum()
bins = np.arange(256)
fn_min = np.inf
thresh = -1
for i in range(1,256):
    p1,p2 = np.hsplit(hist_norm,[i]) # probabilities
    q1,q2 = Q[i],Q[255]-Q[i] # cum sum of classes
    if q1 < 1.e-6 or q2 < 1.e-6:
        continue
    b1,b2 = np.hsplit(bins,[i]) # weights
    # finding means and variances
    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2
    v1,v2 = np.sum(((b1-m1)**2)*p1)/q1,np.sum(((b2-m2)**2)*p2)/q2
    # calculates the minimization function
    fn = v1*q1 + v2*q2
    if fn < fn_min:
        fn_min = fn
        thresh = i
#+end_src
The result is a threshold value for the entire image.
*** Morphological Transformations
Typically, all morphological transforms are only applied to thresholded images. This is because they work on the shape of the image itself.
Please see https://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html for great examples of the different transformations available and visual examples of how they work.
**** Erosion - erodes away the foreground; the kernel slides through the image and "A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero)."
**** Dialation - opposite of erosion and now pixel will become white if at least one in the kernel is white
**** Opening - erosion followed by dialation, removes noise
**** Closing - dialation followed by erosion, good for removing holes in foreground objects
**** Morphological gradient - simply the difference between dialtion and erosion, produces an outline of foregrond objects
**** Top hat and black hat - you probably don't need to know these?
*** Distance Transform
**** Background
So, you finally have a thresholded image, but what do we do with it now? You need to apply the distance transform. The distance transform just computes the distance between each pixel and the nearest non-zero pixel.
Here is what is looks like to apply the distance transform
#+DOWNLOADED: screenshot @ 2021-03-26 12:25:01
[[file:images/20210326-122501_screenshot.png]]
Notice how many of the values are above 1 (which for our image data type these values would be clipped). We will have to normalize this output to get any meaningful output. If we tried to show the above distance transform, it would actually just appear to be an inverse of our input!
**** Naive Application
Now, let's try to apply this to a pre-thresholded image:
#+DOWNLOADED: screenshot @ 2021-03-26 12:29:20
[[file:images/20210326-122920_screenshot.png]]
We can apply the DT and plot it in 3D:
#+begin_src python
  M = imread('circles.png')
  imshow(M)
  dd=bwdist(M)
  mesh(dd)
#+end_src
#+DOWNLOADED: screenshot @ 2021-03-26 12:30:53
[[file:images/20210326-123053_screenshot.png]]
WOW! Each of our objects are indeed a "basin" now. However, notice how the bottom of each object is flat. In order for our algorithm to work, we need single drainages for each basin and having flat basins slows down and complicates the algorithm.
**** Improved Application
What we actually want to do is just invert our image and then calculate the DT:
#+DOWNLOADED: screenshot @ 2021-03-26 12:36:04
[[file:images/20210326-123604_screenshot.png]]
Now, the centers of our objects are clear, but they are now hills instead of basins. Just multiply all the values by -1 to fix this:
#+DOWNLOADED: screenshot @ 2021-03-26 12:37:28
[[file:images/20210326-123728_screenshot.png]]
Here is the code:
#+begin_src python
  M = imread('circles.png')
  imshow(M)
  dd= -bwdist(~M)
  mesh(dd)
#+end_src
...and the result:
#+DOWNLOADED: screenshot @ 2021-03-26 12:39:40
[[file:images/20210326-123940_screenshot.png]]
Yay! This is ready for watershed segmentation.
*** Sources
**** Image Filtering and Convolution: https://www.youtube.com/watch?v=1GUgD2SBl9A
**** Unsharp Mask: https://www.youtube.com/watch?v=u_4d51bOsVs
**** Gaussian Filter: https://www.youtube.com/watch?v=xCHbcVUCYBI
**** Median Filter: https://www.youtube.com/watch?v=StX_1iEO3ck
**** Edge Filters: https://www.youtube.com/watch?v=Oy4duAOGdWQ
**** GMM: https://www.youtube.com/watch?v=kkAirywakmk
**** Sobel Edge Detection: https://www.youtube.com/watch?v=uihBwtPIBxM
**** Canny Edge Detection: https://www.youtube.com/watch?v=sRFM5IEqR2w
**** C Implementation of Watershed: https://perso.esiee.fr/~coupriem/tw/index.html
**** Thresholding: https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html
**** Morphological Transformations: https://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html
**** TODO Implementing the Watershed Algorithm: https://www.youtube.com/watch?v=Mpsq82U_qUg
This video has a lot of pointers of how to actually use the algorithm in practice
**** Distance Transform OpenCV: https://docs.opencv.org/3.4/d2/dbd/tutorial_distance_transform.html
**** Watershed Algorithm OpenCV: https://docs.opencv.org/3.4/d3/db4/tutorial_py_watershed.html
**** Broad overview of Watershed: https://www.youtube.com/watch?v=K5P5rjDiZzk

